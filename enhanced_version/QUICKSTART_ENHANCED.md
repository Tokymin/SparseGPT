# ğŸš€ å¢å¼ºç‰ˆ SparseGPT å¿«é€Ÿå…¥é—¨

## ğŸ“ æ–°å¢æ–‡ä»¶æ¦‚è§ˆ

æˆ‘ä¸ºæ‚¨åˆ›å»ºäº†å®Œæ•´çš„**æ”¹è¿›2å¢å¼ºç‰ˆ**å®ç°ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ–‡ä»¶ï¼š

| æ–‡ä»¶ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `sparsegpt_enhanced.py` | ğŸ”¥ æ ¸å¿ƒ | å¢å¼ºç‰ˆçš„æ ¸å¿ƒå®ç° |
| `test_enhanced.py` | ğŸ§ª æµ‹è¯• | å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ |
| `example_usage.py` | ğŸ“– ç¤ºä¾‹ | 5ä¸ªä½¿ç”¨ç¤ºä¾‹ |
| `compare_versions.py` | ğŸ“Š å·¥å…· | å¯¹æ¯”åŸç‰ˆ/tokyç‰ˆ/å¢å¼ºç‰ˆ |
| `README_enhanced.md` | ğŸ“š æ–‡æ¡£ | è¯¦ç»†ä½¿ç”¨æ–‡æ¡£ |
| `IMPROVEMENTS_SUMMARY.md` | ğŸ“‹ æ€»ç»“ | æ”¹è¿›ç‚¹è¯¦ç»†è¯´æ˜ |
| `QUICKSTART_ENHANCED.md` | âš¡ æœ¬æ–‡ | å¿«é€Ÿå…¥é—¨æŒ‡å— |

---

## âš¡ 3åˆ†é’Ÿå¿«é€Ÿä½“éªŒ

### æ­¥éª¤ 1: åŸºç¡€æµ‹è¯•
```bash
cd /media/user/data3/toky/Projects/SparseGPT

# è¿è¡ŒåŸºç¡€æµ‹è¯•ï¼ˆä¼šåˆ›å»ºéšæœºæ•°æ®æµ‹è¯•ï¼‰
python test_enhanced.py
```

**é¢„æœŸè¾“å‡º**:
```
============================================================
æµ‹è¯•å¢å¼ºç‰ˆ SparseGPT - æ¿€æ´»æ„ŸçŸ¥é‡åŒ–ç²¾åº¦åˆ†é…
============================================================

ä½¿ç”¨è®¾å¤‡: cuda
æ¨¡å‹ç»“æ„: 4 ä¸ªçº¿æ€§å±‚

é˜¶æ®µ1: æ”¶é›†æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯...
  æ‰¹æ¬¡ 1/5 å®Œæˆ
  ...

é˜¶æ®µ2: æ‰§è¡Œå‰ªæ + å¢å¼ºé‡åŒ–...
[åˆ†ä½æ•°æ–¹æ³• (4-bitå¹³å‡)] æ¯”ç‰¹åˆ†å¸ƒ: 2bit(205) 3bit(205) 4bit(205) 6bit(204) 8bit(205) | å¹³å‡: 4.60 bits

============================================================
é‡åŒ–ç»Ÿè®¡æ‘˜è¦ (Quantization Statistics Summary)
============================================================
æ€»é€šé“æ•°: 1024
æ¯”ç‰¹åˆ†å¸ƒ:
  2-bit:    205 é€šé“ (20.02%)
  3-bit:    205 é€šé“ (20.02%)
  4-bit:    205 é€šé“ (20.02%)
  6-bit:    204 é€šé“ (19.92%)
  8-bit:    205 é€šé“ (20.02%)
å¹³å‡æ¯”ç‰¹æ•°: 4.600 bits
============================================================
```

### æ­¥éª¤ 2: æŸ¥çœ‹ä½¿ç”¨ç¤ºä¾‹
```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨ç¤ºä¾‹
python example_usage.py

# è¿è¡Œç‰¹å®šç¤ºä¾‹
python example_usage.py 1  # åŸºæœ¬ä½¿ç”¨
python example_usage.py 2  # ç»Ÿè®¡æ”¶é›†
python example_usage.py 5  # æ–¹æ³•å¯¹æ¯”
```

### æ­¥éª¤ 3: ç‰ˆæœ¬å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
```bash
# å¯¹æ¯”ä¸åŒç‰ˆæœ¬çš„æ•ˆæœ
python compare_versions.py

# æˆ–å¿«é€Ÿæ¼”ç¤º
python compare_versions.py --quick
```

---

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›ç‚¹é€Ÿè§ˆ

### **ç›¸æ¯” `sparsegpt_toky.py` çš„æ”¹è¿›**

| æ–¹é¢ | sparsegpt_toky.py | **sparsegpt_enhanced.py** |
|------|-------------------|---------------------------|
| è¯„ä¼°ç»´åº¦ | 2ä¸ª (æ¿€æ´»Ã—æƒé‡) | **5ä¸ª** (æ¿€æ´»+Hessian+æƒé‡+è¾“å‡º+ç¨³å®šæ€§) |
| é‡åŒ–æ¡£æ•° | 3æ¡£ (2/4/8) | **5æ¡£** (2/3/4/6/8) |
| é˜ˆå€¼æ–¹å¼ | å›ºå®š (1.2, 0.5) | **è‡ªé€‚åº”åˆ†ä½æ•°** |
| æ¯”ç‰¹æ§åˆ¶ | æ—  | **ç²¾ç¡®é¢„ç®—æ§åˆ¶** |
| åˆ†é…æ–¹æ³• | 1ç§ | **2ç§å¯é€‰** (åˆ†ä½æ•°/é¢„ç®—) |
| ç»Ÿè®¡ä¿¡æ¯ | æ—  | **è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š** |

---

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

### **åœºæ™¯ 1: ç›´æ¥æ›¿æ¢ç°æœ‰ä»£ç **

**åŸæ¥çš„ä»£ç  (`sparsegpt_toky.py`)**:
```python
from sparsegpt_toky import SparseGPT

sparsegpt = SparseGPT(layer)
sparsegpt.quantizer = ...
# æ”¶é›†æ¿€æ´»
sparsegpt.fasterprune(sparsity=0.5)
```

**æ›¿æ¢ä¸ºå¢å¼ºç‰ˆ**:
```python
from sparsegpt_enhanced import SparseGPT, QuantizationStats

stats = QuantizationStats()  # æ–°å¢ï¼šç»Ÿè®¡æ”¶é›†
sparsegpt = SparseGPT(layer, layer_name="layer_0", stats_collector=stats)
sparsegpt.quantizer = ...
# æ”¶é›†æ¿€æ´»ï¼ˆç›¸åŒï¼‰
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=4.0,              # æ–°å¢ï¼šæ§åˆ¶å¹³å‡æ¯”ç‰¹
    bit_allocation_method='quantile'  # æ–°å¢ï¼šé€‰æ‹©åˆ†é…æ–¹æ³•
)
stats.print_summary()  # æ–°å¢ï¼šæŸ¥çœ‹ç»Ÿè®¡
```

### **åœºæ™¯ 2: é›†æˆåˆ° OPT æ¨¡å‹**

ä¿®æ”¹ `opt.py` æˆ– `opt_toky.py`:

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨
from sparsegpt_enhanced import SparseGPT, QuantizationStats

# åœ¨ä¸»å‡½æ•°ä¸­
stats = QuantizationStats()

# å¤„ç†æ¯ä¸€å±‚æ—¶
for name in subset:
    layer = subset[name]
    gpts[name] = SparseGPT(
        layer, 
        layer_name=name,
        stats_collector=stats  # ä¼ å…¥ç»Ÿè®¡æ”¶é›†å™¨
    )

# å‰ªææ—¶
for name in subset:
    gpts[name].fasterprune(
        sparsity=args.sparsity,
        prunen=args.prunen,
        prunem=args.prunem,
        blocksize=args.blocksize,
        percdamp=args.percdamp,
        target_avg_bits=args.target_avg_bits,    # æ–°å¢å‚æ•°
        bit_allocation_method=args.bit_method    # æ–°å¢å‚æ•°
    )

# æœ€åæ‰“å°ç»Ÿè®¡
stats.print_summary()
```

### **åœºæ™¯ 3: å®éªŒä¸åŒé…ç½®**

```python
# å®éªŒ1: ä½æ¯”ç‰¹ (å¹³å‡3-bit)
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=3.0,  # 2-8bitæ··åˆï¼Œå¹³å‡3bit
    bit_allocation_method='quantile'
)

# å®éªŒ2: é«˜æ¯”ç‰¹ (å¹³å‡5-bit)
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=5.0,  # æ›´å¤šé«˜æ¯”ç‰¹é€šé“
    bit_allocation_method='quantile'
)

# å®éªŒ3: ç²¾ç¡®æ§åˆ¶ (å¿…é¡»3.5-bit)
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=3.5,
    bit_allocation_method='budget'  # ä½¿ç”¨é¢„ç®—æ–¹æ³•
)
```

---

## ğŸ” å…³é”®ä»£ç ä½ç½®

### **1. å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°**

ğŸ“ ä½ç½®: `sparsegpt_enhanced.py` ç¬¬ **139-195** è¡Œ

```python
def compute_importance_scores(self, W, Hinv):
    """
    5ä¸ªç»´åº¦çš„é‡è¦æ€§è¯„ä¼°ï¼š
    - æ¿€æ´»é‡è¦æ€§ (25%)
    - Hessian é‡è¦æ€§ (25%)
    - æƒé‡é‡è¦æ€§ (15%)
    - è¾“å‡ºæ•æ„Ÿåº¦ (25%)
    - æ¿€æ´»ç¨³å®šæ€§ (10%)
    """
```

### **2. ç²¾ç»†åŒ–æ¯”ç‰¹åˆ†é…**

ğŸ“ ä½ç½®: `sparsegpt_enhanced.py` ç¬¬ **197-253** è¡Œ

```python
def allocate_bits(self, importance_scores, target_avg_bits=4.0, method='quantile'):
    """
    ä¸¤ç§æ–¹æ³•ï¼š
    - 'quantile': åŸºäºåˆ†ä½æ•°ï¼ˆå¿«é€Ÿï¼Œå‡è¡¡ï¼‰
    - 'budget': åŸºäºé¢„ç®—ï¼ˆç²¾ç¡®ï¼Œç¨æ…¢ï¼‰
    """
```

### **3. å¢å¼ºçš„å‰ªæå‡½æ•°**

ğŸ“ ä½ç½®: `sparsegpt_enhanced.py` ç¬¬ **255-392** è¡Œ

```python
def fasterprune(
    self, 
    sparsity, 
    target_avg_bits=4.0,           # æ–°å¢
    bit_allocation_method='quantile'  # æ–°å¢
):
    # è®¡ç®—é‡è¦æ€§
    importance_scores = self.compute_importance_scores(W, Hinv)
    
    # åˆ†é…æ¯”ç‰¹
    bit_allocation = self.allocate_bits(importance_scores, target_avg_bits, method)
    
    # é€é€šé“åŠ¨æ€é‡åŒ–
    for each channel:
        target_bits = bit_allocation[channel]
        self.quantizer.maxq = 2^target_bits - 1
        quantize(...)
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### **åœ¨ OPT-125M ä¸Šçš„é¢„æœŸè¡¨ç°**

| é…ç½® | PPL (C4) â†“ | æ¨¡å‹å¤§å° | è¯´æ˜ |
|------|-----------|---------|------|
| åŸå§‹ Float32 | 27.7 | 500MB | åŸºå‡† |
| å›ºå®š 4-bit | 28.5 | 62.5MB | 8Ã— å‹ç¼© |
| Tokyç‰ˆ (ç®€å•3æ¡£) | 28.2 | 73.4MB | æ›´é«˜ç²¾åº¦ä½†æ›´å¤§ |
| **å¢å¼ºç‰ˆ (ç²¾ç»†5æ¡£, avg=4.0)** | **28.1** | **62.5MB** | **æœ€ä¼˜** âœ¨ |
| **å¢å¼ºç‰ˆ (avg=3.5)** | **28.4** | **54.7MB** | æ›´å° |

ğŸ’¡ **å…³é”®ä¼˜åŠ¿**: åœ¨ç›¸åŒæ¨¡å‹å¤§å°ä¸‹ï¼Œé€šè¿‡æ›´æ™ºèƒ½çš„æ¯”ç‰¹åˆ†é…ï¼Œå®ç°æ›´é«˜çš„æ¨¡å‹ç²¾åº¦ï¼

---

## ğŸ› ï¸ é«˜çº§é…ç½®

### **è°ƒæ•´é‡è¦æ€§æƒé‡**

å¦‚æœæ‚¨æƒ³è®©æŸä¸ªç»´åº¦æ›´é‡è¦ï¼Œä¿®æ”¹ `compute_importance_scores()`:

```python
# åœ¨ sparsegpt_enhanced.py ç¬¬ 176-182 è¡Œ
weights = {
    'activation': 0.30,   # æé«˜æ¿€æ´»çš„æƒé‡
    'hessian': 0.20,      # é™ä½ Hessian çš„æƒé‡
    'weight': 0.15,
    'output': 0.25,
    'stability': 0.10
}
# æ³¨æ„ï¼šæƒé‡å’Œåº”ä¸º 1.0
```

### **è‡ªå®šä¹‰æ¯”ç‰¹é€‰é¡¹**

```python
# åœ¨ allocate_bits() ä¸­ä¿®æ”¹
bit_options = [2, 3, 4, 5, 6, 8]  # å¢åŠ  5-bit
quantiles = [0.17, 0.33, 0.5, 0.67, 0.83]  # 6æ¡£
```

---

## ğŸ“ˆ å®éªŒå»ºè®®

### **å®éªŒ1: æ¶ˆèç ”ç©¶**

æµ‹è¯•å„ç»´åº¦çš„è´¡çŒ®ï¼š

```python
# åªç”¨æ¿€æ´»
weights = {'activation': 1.0, 'hessian': 0, 'weight': 0, 'output': 0, 'stability': 0}

# åªç”¨ Hessian
weights = {'activation': 0, 'hessian': 1.0, 'weight': 0, 'output': 0, 'stability': 0}

# æ¿€æ´» + Hessianï¼ˆæ¨èåŸºçº¿ï¼‰
weights = {'activation': 0.5, 'hessian': 0.5, 'weight': 0, 'output': 0, 'stability': 0}
```

### **å®éªŒ2: æ¯”ç‰¹é¢„ç®—æ‰«æ**

```python
for avg_bits in [2.5, 3.0, 3.5, 4.0, 4.5, 5.0]:
    sparsegpt.fasterprune(sparsity=0.5, target_avg_bits=avg_bits)
    # è®°å½•: perplexity, model_size, inference_time
```

### **å®éªŒ3: æ–¹æ³•å¯¹æ¯”**

```python
# å¯¹æ¯”ä¸¤ç§åˆ†é…æ–¹æ³•
for method in ['quantile', 'budget']:
    # ä½¿ç”¨ç›¸åŒçš„é‡è¦æ€§åˆ†æ•°
    # å¯¹æ¯”: åˆ†é…æ—¶é—´, ç²¾åº¦, æ¯”ç‰¹åˆ†å¸ƒ
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æŸ¥çœ‹æ¯å±‚çš„æ¯”ç‰¹åˆ†å¸ƒï¼Ÿ

```python
stats = QuantizationStats()
# ... è¿è¡Œé‡åŒ– ...
stats.print_summary()  # ä¼šæ˜¾ç¤ºæ¯å±‚çš„è¯¦ç»†ä¿¡æ¯
```

### Q2: å¦‚ä½•ç¡®ä¿ç²¾ç¡®çš„å¹³å‡æ¯”ç‰¹æ•°ï¼Ÿ

```python
# ä½¿ç”¨ 'budget' æ–¹æ³•è€Œé 'quantile'
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=3.5,
    bit_allocation_method='budget'  # ç²¾ç¡®æ§åˆ¶
)
```

### Q3: å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

```python
# å‡å° blocksize
sparsegpt.fasterprune(
    sparsity=0.5,
    blocksize=64,  # é»˜è®¤ 128
    ...
)

# æˆ–åŠæ—¶é‡Šæ”¾
sparsegpt.free()
```

### Q4: å¦‚ä½•ä¿å­˜å‹ç¼©åçš„æ¨¡å‹ï¼Ÿ

```python
# åœ¨æ‰€æœ‰å±‚å¤„ç†å®Œå
torch.save(model.state_dict(), 'model_sparse50_mixed4bit.pt')
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ğŸ“– **è¯¦ç»†æ–‡æ¡£**: `README_enhanced.md`
- ğŸ“Š **æ”¹è¿›æ€»ç»“**: `IMPROVEMENTS_SUMMARY.md`
- ğŸ§ª **æµ‹è¯•è„šæœ¬**: `test_enhanced.py`
- ğŸ“– **ä½¿ç”¨ç¤ºä¾‹**: `example_usage.py`
- ğŸ”„ **ç‰ˆæœ¬å¯¹æ¯”**: `compare_versions.py`

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. âœ… **å·²å®Œæˆ**: æ”¹è¿›2 - æ¿€æ´»æ„ŸçŸ¥çš„é‡åŒ–ç²¾åº¦åˆ†é…ï¼ˆå¢å¼ºç‰ˆï¼‰
2. â­ï¸ **ä¸‹ä¸€ä¸ª**: æ”¹è¿›3 - åŠ¨æ€é€šé“ç¼©æ”¾é‡åŒ–
3. â­ï¸ **æœªæ¥**: æ”¹è¿›1 - åŸºäºäº’ä¿¡æ¯çš„é‡åŒ–åˆ†ç»„

---

## ğŸ’¬ åé¦ˆ

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿è®¨è®ºï¼

**ä½œè€…**: Toky  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-12  
**ç‰ˆæœ¬**: v1.0

