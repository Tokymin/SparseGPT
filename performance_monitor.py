#!/usr/bin/env python3
"""
SparseGPT æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å·¥å…·
ç”¨äºå®æ—¶ç›‘æ§æ¨¡å‹å‹ç¼©æ•ˆæœå’Œæ€§èƒ½æŒ‡æ ‡
"""

import torch
import time
import psutil
import numpy as np
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import json
from pathlib import Path


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ï¼šè·Ÿè¸ªå‹ç¼©è¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡"""
    
    def __init__(self, model_name: str, save_dir: str = "./monitoring_results"):
        self.model_name = model_name
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        
        # ç›‘æ§æ•°æ®å­˜å‚¨
        self.metrics = {
            'layer_metrics': [],
            'memory_usage': [],
            'compression_ratios': [],
            'quantization_errors': [],
            'timing': []
        }
        
        # åŸºå‡†æ€§èƒ½
        self.baseline_ppl = None
        self.baseline_memory = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        self.baseline_memory = psutil.virtual_memory().used / 1024**3  # GB
        print(f"ğŸ” å¼€å§‹ç›‘æ§ {self.model_name}")
        print(f"ğŸ“Š åŸºå‡†å†…å­˜ä½¿ç”¨: {self.baseline_memory:.2f} GB")
        
    def log_layer_processing(self, layer_idx: int, layer_name: str, 
                           sparsity: float, quantization_bits: int,
                           compression_ratio: float, error: float):
        """è®°å½•å•å±‚å¤„ç†æŒ‡æ ‡"""
        layer_metric = {
            'layer_idx': layer_idx,
            'layer_name': layer_name,
            'sparsity': sparsity,
            'quantization_bits': quantization_bits,
            'compression_ratio': compression_ratio,
            'quantization_error': error,
            'timestamp': time.time() - self.start_time
        }
        self.metrics['layer_metrics'].append(layer_metric)
        
        print(f"ğŸ“ˆ Layer {layer_idx} ({layer_name}): "
              f"Sparsity={sparsity:.3f}, Bits={quantization_bits}, "
              f"Compression={compression_ratio:.2f}x, Error={error:.6f}")
    
    def log_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        current_memory = psutil.virtual_memory().used / 1024**3
        gpu_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        
        memory_info = {
            'cpu_memory_gb': current_memory,
            'gpu_memory_gb': gpu_memory,
            'timestamp': time.time() - self.start_time
        }
        self.metrics['memory_usage'].append(memory_info)
        
    def log_perplexity(self, dataset: str, perplexity: float, is_baseline: bool = False):
        """è®°å½•å›°æƒ‘åº¦ç»“æœ"""
        if is_baseline:
            self.baseline_ppl = perplexity
            print(f"ğŸ“Š åŸºå‡†å›°æƒ‘åº¦ ({dataset}): {perplexity:.3f}")
        else:
            improvement = ((self.baseline_ppl - perplexity) / self.baseline_ppl * 100) if self.baseline_ppl else 0
            print(f"ğŸ“Š å‹ç¼©åå›°æƒ‘åº¦ ({dataset}): {perplexity:.3f} "
                  f"(å˜åŒ–: {improvement:+.2f}%)")
    
    def calculate_compression_stats(self, model) -> Dict:
        """è®¡ç®—æ•´ä½“å‹ç¼©ç»Ÿè®¡"""
        total_params = sum(p.numel() for p in model.parameters())
        sparse_params = sum(torch.sum(p == 0).item() for p in model.parameters() if 'weight' in str(p))
        
        # è®¡ç®—é‡åŒ–èŠ‚çœ
        quantized_layers = 0
        total_quantization_bits = 0
        for name, param in model.named_parameters():
            if 'weight' in name and hasattr(param, 'quantization_bits'):
                quantized_layers += 1
                total_quantization_bits += param.quantization_bits
        
        avg_bits = total_quantization_bits / max(quantized_layers, 1)
        
        stats = {
            'total_parameters': total_params,
            'sparse_parameters': sparse_params,
            'sparsity_ratio': sparse_params / total_params,
            'quantized_layers': quantized_layers,
            'average_bits': avg_bits,
            'estimated_compression': 32 / avg_bits if avg_bits > 0 else 1.0
        }
        
        return stats
    
    def generate_report(self, model, final_ppl: Dict[str, float]):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        stats = self.calculate_compression_stats(model)
        
        report = {
            'model_name': self.model_name,
            'processing_time': time.time() - self.start_time,
            'compression_stats': stats,
            'final_perplexity': final_ppl,
            'layer_metrics': self.metrics['layer_metrics'],
            'memory_usage': self.metrics['memory_usage']
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.save_dir / f"{self.model_name.replace('/', '_')}_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š å‹ç¼©æ€§èƒ½æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ·ï¸  æ¨¡å‹: {self.model_name}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {report['processing_time']:.2f}ç§’")
        print(f"ğŸ—œï¸  ç¨€ç–åº¦: {stats['sparsity_ratio']:.1%}")
        print(f"ğŸ”¢ é‡åŒ–å±‚æ•°: {stats['quantized_layers']}")
        print(f"ğŸ“ å¹³å‡æ¯”ç‰¹: {stats['average_bits']:.1f}")
        print(f"ğŸ“¦ ä¼°è®¡å‹ç¼©æ¯”: {stats['estimated_compression']:.1f}x")
        
        for dataset, ppl in final_ppl.items():
            print(f"ğŸ“ˆ {dataset} å›°æƒ‘åº¦: {ppl:.3f}")
        
        print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        print("="*60)
        
        return report
    
    def plot_metrics(self):
        """ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡å›¾è¡¨"""
        if not self.metrics['layer_metrics']:
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Performance Metrics - {self.model_name}', fontsize=16)
        
        # ç¨€ç–åº¦å˜åŒ–
        layers = [m['layer_idx'] for m in self.metrics['layer_metrics']]
        sparsities = [m['sparsity'] for m in self.metrics['layer_metrics']]
        axes[0, 0].plot(layers, sparsities, 'b-o')
        axes[0, 0].set_title('Sparsity by Layer')
        axes[0, 0].set_xlabel('Layer Index')
        axes[0, 0].set_ylabel('Sparsity Ratio')
        axes[0, 0].grid(True)
        
        # å‹ç¼©æ¯”
        compression_ratios = [m['compression_ratio'] for m in self.metrics['layer_metrics']]
        axes[0, 1].plot(layers, compression_ratios, 'g-o')
        axes[0, 1].set_title('Compression Ratio by Layer')
        axes[0, 1].set_xlabel('Layer Index')
        axes[0, 1].set_ylabel('Compression Ratio')
        axes[0, 1].grid(True)
        
        # é‡åŒ–è¯¯å·®
        errors = [m['quantization_error'] for m in self.metrics['layer_metrics']]
        axes[1, 0].plot(layers, errors, 'r-o')
        axes[1, 0].set_title('Quantization Error by Layer')
        axes[1, 0].set_xlabel('Layer Index')
        axes[1, 0].set_ylabel('Error')
        axes[1, 0].grid(True)
        
        # å†…å­˜ä½¿ç”¨
        if self.metrics['memory_usage']:
            timestamps = [m['timestamp'] for m in self.metrics['memory_usage']]
            cpu_memory = [m['cpu_memory_gb'] for m in self.metrics['memory_usage']]
            axes[1, 1].plot(timestamps, cpu_memory, 'purple', label='CPU Memory')
            if torch.cuda.is_available():
                gpu_memory = [m['gpu_memory_gb'] for m in self.metrics['memory_usage']]
                axes[1, 1].plot(timestamps, gpu_memory, 'orange', label='GPU Memory')
            axes[1, 1].set_title('Memory Usage Over Time')
            axes[1, 1].set_xlabel('Time (seconds)')
            axes[1, 1].set_ylabel('Memory (GB)')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        plot_path = self.save_dir / f"{self.model_name.replace('/', '_')}_metrics.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")


def optimize_quantization_parameters(model, monitor: PerformanceMonitor):
    """åŠ¨æ€ä¼˜åŒ–é‡åŒ–å‚æ•°"""
    print("ğŸ”§ å¼€å§‹åŠ¨æ€ä¼˜åŒ–é‡åŒ–å‚æ•°...")
    
    # åˆ†ææ¯å±‚çš„æ¿€æ´»åˆ†å¸ƒ
    layer_activations = {}
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ¿€æ´»åˆ†å¸ƒåˆ†æ
            pass
    
    # æ ¹æ®æ¿€æ´»åˆ†å¸ƒè°ƒæ•´é‡åŒ–å‚æ•°
    optimization_suggestions = []
    
    for name, param in model.named_parameters():
        if 'weight' in name:
            # åˆ†ææƒé‡åˆ†å¸ƒ
            weight_std = torch.std(param).item()
            weight_mean = torch.mean(torch.abs(param)).item()
            
            # å»ºè®®é‡åŒ–æ¯”ç‰¹æ•°
            if weight_std > 0.1:  # é«˜æ–¹å·®ï¼Œéœ€è¦æ›´å¤šæ¯”ç‰¹
                suggested_bits = 8
            elif weight_std > 0.05:  # ä¸­ç­‰æ–¹å·®
                suggested_bits = 4
            else:  # ä½æ–¹å·®ï¼Œå¯ä»¥ç”¨æ›´å°‘æ¯”ç‰¹
                suggested_bits = 2
                
            optimization_suggestions.append({
                'layer': name,
                'current_std': weight_std,
                'suggested_bits': suggested_bits
            })
    
    print("ğŸ’¡ é‡åŒ–ä¼˜åŒ–å»ºè®®:")
    for suggestion in optimization_suggestions[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"  {suggestion['layer']}: å»ºè®® {suggestion['suggested_bits']}bit "
              f"(å½“å‰std: {suggestion['current_std']:.4f})")
    
    return optimization_suggestions


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    monitor = PerformanceMonitor("facebook/opt-125m")
    monitor.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›æ•°æ®
    for i in range(12):
        monitor.log_layer_processing(
            layer_idx=i,
            layer_name=f"layer_{i}",
            sparsity=0.1 * i,
            quantization_bits=4,
            compression_ratio=2.0 + 0.1 * i,
            error=0.001 * i
        )
        monitor.log_memory_usage()
    
    # ç”ŸæˆæŠ¥å‘Š
    final_ppl = {"wikitext2": 25.953, "ptb": 38.985}
    report = monitor.generate_report(None, final_ppl)
    monitor.plot_metrics()
