#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯ SparseGPT è¿è¡ŒçŠ¶æ€
"""

import torch
import time
from opt_toky import get_opt
from datautils import get_loaders


def quick_test():
    """å¿«é€Ÿæµ‹è¯•å½“å‰æ¨¡å‹çŠ¶æ€"""
    print("ğŸ” å¿«é€ŸçŠ¶æ€æ£€æŸ¥...")
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"ğŸ”¢ CUDAç‰ˆæœ¬: {torch.version.cuda}")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    print("\nğŸ“¥ æµ‹è¯•æ¨¡å‹åŠ è½½...")
    try:
        model = get_opt("facebook/opt-125m")
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model.config.model_type}")
        print(f"ğŸ“Š å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"ğŸ—ï¸  å±‚æ•°: {len(model.model.decoder.layers)}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        dataloader, testloader = get_loaders(
            "wikitext2", nsamples=32, seed=0,
            model="facebook/opt-125m", seqlen=model.seqlen
        )
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"ğŸ“ˆ æ ¡å‡†æ ·æœ¬æ•°: {len(dataloader)}")
        print(f"ğŸ§ª æµ‹è¯•æ ·æœ¬æ•°: {testloader.input_ids.shape}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸš€ æµ‹è¯•å‰å‘ä¼ æ’­...")
    try:
        model.eval()
        with torch.no_grad():
            # æµ‹è¯•ä¸€ä¸ªå°æ‰¹æ¬¡
            sample_input = torch.randint(0, 1000, (1, 10)).to(device)
            start_time = time.time()
            output = model(sample_input)
            inference_time = time.time() - start_time
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
            print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.logits.shape}")
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥ç¨€ç–åŒ–çŠ¶æ€
    print("\nğŸ—œï¸  æ£€æŸ¥å½“å‰ç¨€ç–åŒ–çŠ¶æ€...")
    total_params = 0
    sparse_params = 0
    
    for name, param in model.named_parameters():
        if 'weight' in name:
            total_params += param.numel()
            sparse_params += torch.sum(param == 0).item()
    
    if total_params > 0:
        current_sparsity = sparse_params / total_params
        print(f"ğŸ“Š å½“å‰ç¨€ç–åº¦: {current_sparsity:.1%}")
        print(f"ğŸ”¢ ç¨€ç–å‚æ•°: {sparse_params:,}")
        print(f"ğŸ“ˆ æ€»å‚æ•°: {total_params:,}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æƒé‡å‚æ•°")
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ...")
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ® GPUå·²åˆ†é…: {allocated:.2f} GB")
        print(f"ğŸ’¾ GPUå·²ç¼“å­˜: {cached:.2f} GB")
    
    print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ æç¤º: å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œæ‚¨çš„ SparseGPT ç¯å¢ƒé…ç½®æ­£ç¡®")


if __name__ == "__main__":
    quick_test()
