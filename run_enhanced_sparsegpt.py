#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ SparseGPT è¿è¡Œè„šæœ¬
é›†æˆæ€§èƒ½ç›‘æ§ã€åŠ¨æ€ä¼˜åŒ–å’Œç»“æœåˆ†æ
"""

import argparse
import time
import torch
import json
from pathlib import Path
from performance_monitor import PerformanceMonitor, optimize_quantization_parameters
from opt_toky import get_opt, opt_sequential, opt_eval
from datautils import get_loaders


def run_enhanced_sparsegpt():
    """è¿è¡Œå¢å¼ºç‰ˆ SparseGPT"""
    parser = argparse.ArgumentParser(description='Enhanced SparseGPT with Performance Monitoring')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('model', type=str, help='Model to load (e.g., facebook/opt-125m)')
    parser.add_argument('dataset', type=str, choices=['wikitext2', 'ptb', 'c4'], 
                       help='Dataset for calibration/evaluation')
    
    # å‹ç¼©å‚æ•°
    parser.add_argument('--sparsity', type=float, default=0.5, help='Target sparsity ratio')
    parser.add_argument('--wbits', type=int, default=4, help='Quantization bits')
    parser.add_argument('--nsamples', type=int, default=128, help='Calibration samples')
    
    # é«˜çº§å‚æ•°
    parser.add_argument('--use-mutual-info', action='store_true', help='Enable mutual information grouping')
    parser.add_argument('--dynamic-scaling', action='store_true', help='Enable dynamic scaling')
    parser.add_argument('--num-clusters', type=int, default=4, help='Number of clusters for grouping')
    
    # ç›‘æ§å‚æ•°
    parser.add_argument('--monitor', action='store_true', help='Enable performance monitoring')
    parser.add_argument('--save-results', type=str, default='./results', help='Save results directory')
    parser.add_argument('--optimize-params', action='store_true', help='Enable parameter optimization')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
    if args.monitor:
        monitor = PerformanceMonitor(args.model, args.save_results)
        monitor.start_monitoring()
    else:
        monitor = None
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“Š æ¨¡å‹: {args.model}")
    print(f"ğŸ“ˆ æ•°æ®é›†: {args.dataset}")
    print(f"ğŸ—œï¸  ç›®æ ‡ç¨€ç–åº¦: {args.sparsity}")
    print(f"ğŸ”¢ é‡åŒ–æ¯”ç‰¹: {args.wbits}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model = get_opt(args.model)
    model.eval()
    
    # è®°å½•åŸºå‡†æ€§èƒ½
    if monitor:
        baseline_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        print(f"ğŸ“Š åŸºå‡†å†…å­˜ä½¿ç”¨: {baseline_memory:.2f} GB")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ ¡å‡†æ•°æ®...")
    dataloader, testloader = get_loaders(
        args.dataset, nsamples=args.nsamples, seed=0,
        model=args.model, seqlen=model.seqlen
    )
    
    # æ‰§è¡Œå‹ç¼©
    if args.sparsity > 0:
        print("ğŸ”§ å¼€å§‹ç¨€ç–åŒ–å’Œé‡åŒ–...")
        start_time = time.time()
        
        # åˆ›å»ºå‚æ•°å¯¹è±¡
        class Args:
            def __init__(self):
                self.sparsity = args.sparsity
                self.wbits = args.wbits
                self.nsamples = args.nsamples
                self.use_mutual_info = args.use_mutual_info
                self.dynamic_scaling = args.dynamic_scaling
                self.num_clusters = args.num_clusters
                self.prunen = 0
                self.prunem = 0
                self.percdamp = 0.01
                self.blocksize = 128
                self.gmp = False
                self.minlayer = -1
                self.maxlayer = 1000
                self.prune_only = ''
                self.invert = False
                self.log_wandb = False
        
        args_obj = Args()
        
        # æ‰§è¡Œå‹ç¼©
        opt_sequential(model, dataloader, device, args_obj)
        
        compression_time = time.time() - start_time
        print(f"â±ï¸  å‹ç¼©å®Œæˆï¼Œè€—æ—¶: {compression_time:.2f}ç§’")
        
        # è®°å½•å‹ç¼©ç»Ÿè®¡
        if monitor:
            total_params = sum(p.numel() for p in model.parameters())
            sparse_params = sum(torch.sum(p == 0).item() for p in model.parameters() if 'weight' in str(p))
            actual_sparsity = sparse_params / total_params
            
            print(f"ğŸ“Š å®é™…ç¨€ç–åº¦: {actual_sparsity:.1%}")
            print(f"ğŸ—œï¸  å‹ç¼©æ¯”: {1/(1-actual_sparsity):.1f}x")
    
    # å¤šæ•°æ®é›†è¯„ä¼°
    print("ğŸ“ˆ å¼€å§‹è¯„ä¼°...")
    results = {}
    
    for dataset in ['wikitext2', 'ptb']:
        print(f"ğŸ” è¯„ä¼° {dataset}...")
        
        # é‡æ–°åŠ è½½æ•°æ®
        _, testloader = get_loaders(
            dataset, seed=0, model=args.model, seqlen=model.seqlen
        )
        
        # è¯„ä¼°
        ppl = opt_eval(model, testloader, device, dataset, args_obj, False)
        results[dataset] = ppl
        
        if monitor:
            monitor.log_perplexity(dataset, ppl, is_baseline=False)
    
    # å‚æ•°ä¼˜åŒ–å»ºè®®
    if args.optimize_params and monitor:
        print("ğŸ”§ åˆ†æé‡åŒ–å‚æ•°ä¼˜åŒ–æœºä¼š...")
        suggestions = optimize_quantization_parameters(model, monitor)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    if monitor:
        print("ğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        report = monitor.generate_report(model, results)
        monitor.plot_metrics()
        
        # ä¿å­˜æ¨¡å‹
        if args.save_results:
            save_path = Path(args.save_results) / f"{args.model.replace('/', '_')}_compressed"
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹çŠ¶æ€
            torch.save(model.state_dict(), save_path / "model_state.pt")
            
            # ä¿å­˜å‹ç¼©é…ç½®
            config = {
                'model': args.model,
                'sparsity': args.sparsity,
                'wbits': args.wbits,
                'use_mutual_info': args.use_mutual_info,
                'dynamic_scaling': args.dynamic_scaling,
                'results': results
            }
            with open(save_path / "config.json", 'w') as f:
                json.dump(config, f, indent=2)
            
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    print("ğŸ‰ å‹ç¼©å®Œæˆï¼æœ€ç»ˆç»“æœ:")
    print("="*60)
    for dataset, ppl in results.items():
        print(f"ğŸ“Š {dataset}: {ppl:.3f}")
    print("="*60)


if __name__ == "__main__":
    run_enhanced_sparsegpt()
