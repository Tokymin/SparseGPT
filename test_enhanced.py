"""
æµ‹è¯•å¢å¼ºç‰ˆ SparseGPT çš„ç¤ºä¾‹è„šæœ¬

æ¼”ç¤º:
1. å¦‚ä½•ä½¿ç”¨å¢å¼ºç‰ˆçš„æ¿€æ´»æ„ŸçŸ¥é‡åŒ–
2. å¦‚ä½•æ”¶é›†å’Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
3. å¯¹æ¯”ä¸åŒæ¯”ç‰¹åˆ†é…æ–¹æ³•çš„æ•ˆæœ
"""

import torch
import torch.nn as nn
from sparsegpt_enhanced import SparseGPT, QuantizationStats


def create_test_model():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    model = nn.Sequential(
        nn.Linear(512, 1024),
        nn.ReLU(),
        nn.Linear(1024, 2048),
        nn.ReLU(),
        nn.Linear(2048, 1024),
        nn.ReLU(),
        nn.Linear(1024, 512)
    )
    return model


def test_enhanced_quantization():
    """æµ‹è¯•å¢å¼ºç‰ˆé‡åŒ–"""
    print("="*80)
    print("æµ‹è¯•å¢å¼ºç‰ˆ SparseGPT - æ¿€æ´»æ„ŸçŸ¥é‡åŒ–ç²¾åº¦åˆ†é…")
    print("="*80)
    
    # è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_test_model().to(device)
    print(f"æ¨¡å‹ç»“æ„: {len([m for m in model.modules() if isinstance(m, nn.Linear)])} ä¸ªçº¿æ€§å±‚")
    
    # åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨
    stats_collector = QuantizationStats()
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    seq_len = 128
    hidden_dim = 512
    num_batches = 5
    
    print(f"\nç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: batch_size={batch_size}, seq_len={seq_len}, hidden_dim={hidden_dim}")
    
    # æµ‹è¯•ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
    layer = model[0]  # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
    layer_name = "model.0.linear"
    
    print(f"\nå¤„ç†å±‚: {layer_name}")
    print(f"æƒé‡å½¢çŠ¶: {layer.weight.shape}")
    
    # åˆ›å»º SparseGPT å®ä¾‹
    sparsegpt = SparseGPT(layer, layer_name=layer_name, stats_collector=stats_collector)
    
    # æ¨¡æ‹Ÿé‡åŒ–å™¨
    from quant import Quantizer
    sparsegpt.quantizer = Quantizer()
    sparsegpt.quantizer.configure(bits=4, perchannel=True, sym=True)
    
    print("\né˜¶æ®µ1: æ”¶é›†æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯...")
    # æ”¶é›†å¤šä¸ªæ‰¹æ¬¡çš„ç»Ÿè®¡ä¿¡æ¯
    for batch_idx in range(num_batches):
        # ç”Ÿæˆéšæœºè¾“å…¥ï¼ˆæ¨¡æ‹ŸçœŸå®æ•°æ®åˆ†å¸ƒï¼‰
        inp = torch.randn(batch_size, seq_len, hidden_dim, device=device)
        out = layer(inp.reshape(-1, hidden_dim)).reshape(batch_size, seq_len, -1)
        
        sparsegpt.add_batch(inp, out)
        print(f"  æ‰¹æ¬¡ {batch_idx+1}/{num_batches} å®Œæˆ")
    
    print("\né˜¶æ®µ2: æ‰§è¡Œå‰ªæ + å¢å¼ºé‡åŒ–...")
    
    # æµ‹è¯•ä¸åŒçš„é…ç½®
    configs = [
        {
            'name': 'åˆ†ä½æ•°æ–¹æ³• (4-bitå¹³å‡)',
            'sparsity': 0.5,
            'target_avg_bits': 4.0,
            'method': 'quantile'
        },
        {
            'name': 'åˆ†ä½æ•°æ–¹æ³• (3-bitå¹³å‡)',
            'sparsity': 0.5,
            'target_avg_bits': 3.0,
            'method': 'quantile'
        },
        {
            'name': 'é¢„ç®—æ–¹æ³• (4-bitå¹³å‡)',
            'sparsity': 0.5,
            'target_avg_bits': 4.0,
            'method': 'budget'
        }
    ]
    
    for config in configs:
        print(f"\n{'='*60}")
        print(f"é…ç½®: {config['name']}")
        print(f"{'='*60}")
        
        # é‡æ–°åˆ›å»ºå®ä¾‹ï¼ˆé¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
        sparsegpt_test = SparseGPT(layer, layer_name=config['name'], stats_collector=stats_collector)
        sparsegpt_test.quantizer = Quantizer()
        sparsegpt_test.quantizer.configure(bits=4, perchannel=True, sym=True)
        
        # æ”¶é›†ç»Ÿè®¡
        for batch_idx in range(num_batches):
            inp = torch.randn(batch_size, seq_len, hidden_dim, device=device)
            out = layer(inp.reshape(-1, hidden_dim)).reshape(batch_size, seq_len, -1)
            sparsegpt_test.add_batch(inp, out)
        
        # æ‰§è¡Œå‰ªæ + é‡åŒ–
        sparsegpt_test.fasterprune(
            sparsity=config['sparsity'],
            target_avg_bits=config['target_avg_bits'],
            bit_allocation_method=config['method']
        )
        
        sparsegpt_test.free()
    
    # æ‰“å°æ•´ä½“ç»Ÿè®¡
    stats_collector.print_summary()
    
    print("\næµ‹è¯•å®Œæˆ! âœ…")


def compare_with_original():
    """å¯¹æ¯”åŸç‰ˆå’Œå¢å¼ºç‰ˆçš„æ•ˆæœ"""
    print("\n" + "="*80)
    print("å¯¹æ¯”æµ‹è¯•: åŸç‰ˆ vs å¢å¼ºç‰ˆ")
    print("="*80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„å±‚
    layer_original = nn.Linear(512, 1024).to(device)
    layer_enhanced = nn.Linear(512, 1024).to(device)
    
    # å¤åˆ¶æƒé‡
    layer_enhanced.load_state_dict(layer_original.state_dict())
    
    print("\n1. æµ‹è¯•åŸç‰ˆ (å›ºå®š4-bit)...")
    # è¿™é‡Œéœ€è¦åŸç‰ˆçš„ sparsegpt.py
    # from sparsegpt import SparseGPT as SparseGPT_Original
    # ... (ç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…éœ€è¦å®Œæ•´å®ç°)
    
    print("\n2. æµ‹è¯•å¢å¼ºç‰ˆ (æ··åˆç²¾åº¦2/3/4/6/8-bit)...")
    stats = QuantizationStats()
    sparsegpt = SparseGPT(layer_enhanced, layer_name="enhanced", stats_collector=stats)
    
    # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†
    for _ in range(3):
        inp = torch.randn(32, 128, 512, device=device)
        out = layer_enhanced(inp.reshape(-1, 512)).reshape(32, 128, -1)
        sparsegpt.add_batch(inp, out)
    
    # é…ç½®é‡åŒ–å™¨
    from quant import Quantizer
    sparsegpt.quantizer = Quantizer()
    sparsegpt.quantizer.configure(bits=4, perchannel=True, sym=True)
    
    # æ‰§è¡Œå¢å¼ºé‡åŒ–
    sparsegpt.fasterprune(
        sparsity=0.5,
        target_avg_bits=4.0,
        bit_allocation_method='quantile'
    )
    
    stats.print_summary()
    
    print("\nå¯¹æ¯”å®Œæˆ! âœ…")


def analyze_importance_scores():
    """åˆ†æé‡è¦æ€§åˆ†æ•°çš„å„ä¸ªç»„æˆéƒ¨åˆ†"""
    print("\n" + "="*80)
    print("é‡è¦æ€§åˆ†æ•°åˆ†æ")
    print("="*80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    layer = nn.Linear(512, 1024).to(device)
    
    sparsegpt = SparseGPT(layer, layer_name="analysis")
    
    # æ”¶é›†æ¿€æ´»
    print("\næ”¶é›†æ¿€æ´»ç»Ÿè®¡...")
    for _ in range(5):
        inp = torch.randn(32, 128, 512, device=device)
        out = layer(inp.reshape(-1, 512)).reshape(32, 128, -1)
        sparsegpt.add_batch(inp, out)
    
    # è®¡ç®— Hinv
    H = sparsegpt.H
    dead = torch.diag(H) == 0
    H[dead, dead] = 1
    damp = 0.01 * torch.mean(torch.diag(H))
    diag = torch.arange(sparsegpt.columns, device=device)
    H[diag, diag] += damp
    H = torch.linalg.cholesky(H)
    H = torch.cholesky_inverse(H)
    H = torch.linalg.cholesky(H, upper=True)
    Hinv = H
    
    # è®¡ç®—é‡è¦æ€§
    W = layer.weight.data.float()
    importance_scores, component_scores = sparsegpt.compute_importance_scores(W, Hinv)
    
    print("\né‡è¦æ€§åˆ†æ•°ç»Ÿè®¡:")
    print(f"  æ€»ä½“èŒƒå›´: [{importance_scores.min():.3f}, {importance_scores.max():.3f}]")
    print(f"  å¹³å‡å€¼: {importance_scores.mean():.3f}")
    print(f"  æ ‡å‡†å·®: {importance_scores.std():.3f}")
    
    print("\nå„ç»„æˆéƒ¨åˆ†ç»Ÿè®¡:")
    for name, scores in component_scores.items():
        print(f"  {name:12s}: èŒƒå›´=[{scores.min():.3f}, {scores.max():.3f}], "
              f"å‡å€¼={scores.mean():.3f}, æ ‡å‡†å·®={scores.std():.3f}")
    
    print("\nåˆ†æå®Œæˆ! âœ…")


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    print("\nğŸš€ SparseGPT Enhanced - æµ‹è¯•å¥—ä»¶\n")
    
    # è¿è¡Œæµ‹è¯•
    try:
        # æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½
        test_enhanced_quantization()
        
        # æµ‹è¯•2: é‡è¦æ€§åˆ†æ
        analyze_importance_scores()
        
        # æµ‹è¯•3: å¯¹æ¯”æµ‹è¯• (å¯é€‰)
        # compare_with_original()
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*80)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("="*80)

