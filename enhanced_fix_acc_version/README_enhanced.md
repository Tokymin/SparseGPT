# SparseGPT Enhanced - æ”¹è¿›2ï¼šæ¿€æ´»æ„ŸçŸ¥çš„é‡åŒ–ç²¾åº¦åˆ†é…

## ðŸ“‹ æ¦‚è¿°

è¿™æ˜¯ SparseGPT çš„å¢žå¼ºç‰ˆæœ¬ï¼Œå®žçŽ°äº†**æ¿€æ´»æ„ŸçŸ¥çš„æ··åˆç²¾åº¦é‡åŒ–**ã€‚ç›¸æ¯”åŽŸç‰ˆå’Œ `sparsegpt_toky.py`ï¼Œæœ¬ç‰ˆæœ¬æä¾›äº†æ›´ç²¾ç»†å’Œæ™ºèƒ½çš„é‡åŒ–ç­–ç•¥ã€‚

## ðŸŽ¯ æ ¸å¿ƒæ”¹è¿›

### 1. **å¤šç»´åº¦é‡è¦æ€§è¯„ä¼°**

åŽŸç‰ˆåªè€ƒè™‘æƒé‡å’Œ Hessianï¼Œå¢žå¼ºç‰ˆç»¼åˆè€ƒè™‘ **5 ä¸ªç»´åº¦**ï¼š

| ç»´åº¦ | è®¡ç®—æ–¹æ³• | æƒé‡ | è¯´æ˜Ž |
|------|---------|------|------|
| æ¿€æ´»é‡è¦æ€§ | `mean(\|activation\|)` | 25% | è¾“å…¥ç‰¹å¾çš„å¹³å‡å¹…å€¼ |
| Hessian é‡è¦æ€§ | `diag(H^-1)` | 25% | äºŒé˜¶å¯¼æ•°ä¿¡æ¯ï¼ˆå‚æ•°æ•æ„Ÿåº¦ï¼‰ |
| æƒé‡é‡è¦æ€§ | `mean(\|W\|)` | 15% | æƒé‡çš„å¹³å‡å¹…å€¼ |
| è¾“å‡ºæ•æ„Ÿåº¦ | `sqrt(WÂ² Ã— activationÂ²)` | 25% | æƒé‡å¯¹è¾“å‡ºçš„å®žé™…è´¡çŒ® |
| æ¿€æ´»ç¨³å®šæ€§ | `var(activation)` | 10% | æ¿€æ´»çš„æ–¹å·®ï¼ˆåŠ¨æ€èŒƒå›´ï¼‰ |

**ä»£ç ä½ç½®**: `sparsegpt_enhanced.py` ç¬¬ 139-195 è¡Œ

```python
importance_scores = (
    0.25 * act_importance +      # æ¿€æ´»é‡è¦æ€§
    0.25 * hessian_importance +  # Hessian é‡è¦æ€§  
    0.15 * weight_importance +   # æƒé‡é‡è¦æ€§
    0.25 * output_sensitivity +  # è¾“å‡ºæ•æ„Ÿåº¦
    0.10 * activation_stability  # æ¿€æ´»ç¨³å®šæ€§
)
```

### 2. **ç²¾ç»†åŒ–æ¯”ç‰¹åˆ†é…ï¼ˆ5æ¡£ï¼‰**

**åŽŸç‰ˆ (`sparsegpt_toky.py`)**:
```python
if contrib_norm[i] > 1.2:  # 8bit
elif contrib_norm[i] < 0.5:  # 2bit  
else:  # 4bit
# åªæœ‰3æ¡£ï¼Œé˜ˆå€¼å›ºå®š
```

**å¢žå¼ºç‰ˆ**:
```python
# 5æ¡£ç²¾ç»†åˆ†é…: 2/3/4/6/8 bit
quantiles = [20%, 40%, 60%, 80%]  # åŸºäºŽåˆ†ä½æ•°
bit_allocation = {
    bottom 20%: 2bit,   # æœ€ä¸é‡è¦
    20%-40%:    3bit,
    40%-60%:    4bit,
    60%-80%:    6bit,
    top 20%:    8bit    # æœ€é‡è¦
}
```

**ä¼˜åŠ¿**:
- âœ… è‡ªé€‚åº”é˜ˆå€¼ï¼ˆåŸºäºŽå®žé™…åˆ†å¸ƒï¼‰
- âœ… æ›´ç»†ç²’åº¦ï¼ˆ5æ¡£ vs 3æ¡£ï¼‰
- âœ… å‡è¡¡åˆ†å¸ƒï¼ˆæ¯æ¡£ 20%ï¼‰

### 3. **ä¸¤ç§æ¯”ç‰¹åˆ†é…ç­–ç•¥**

#### æ–¹æ³• 1: åˆ†ä½æ•°æ–¹æ³• (Quantile) âš¡
- **ç‰¹ç‚¹**: ç®€å•é«˜æ•ˆï¼Œç¡®ä¿å‡è¡¡åˆ†å¸ƒ
- **é€‚ç”¨**: é€šç”¨åœºæ™¯ï¼Œå¿«é€Ÿå®žéªŒ
- **æ—¶é—´å¤æ‚åº¦**: O(n log n)

#### æ–¹æ³• 2: é¢„ç®—æ–¹æ³• (Budget) ðŸŽ¯  
- **ç‰¹ç‚¹**: ç²¾ç¡®æŽ§åˆ¶å¹³å‡æ¯”ç‰¹æ•°
- **é€‚ç”¨**: ä¸¥æ ¼æ¯”ç‰¹é¢„ç®—çº¦æŸ
- **æ—¶é—´å¤æ‚åº¦**: O(nÂ²)

```python
# ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•ï¼ˆæŽ¨èï¼‰
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=4.0,
    bit_allocation_method='quantile'
)

# ä½¿ç”¨é¢„ç®—æ–¹æ³•ï¼ˆç²¾ç¡®æŽ§åˆ¶ï¼‰
sparsegpt.fasterprune(
    sparsity=0.5,
    target_avg_bits=3.5,
    bit_allocation_method='budget'
)
```

### 4. **ç»Ÿè®¡ä¿¡æ¯æ”¶é›†**

å¢žå¼ºç‰ˆæä¾›è¯¦ç»†çš„é‡åŒ–ç»Ÿè®¡ï¼š

```python
stats_collector = QuantizationStats()

# ... è¿è¡Œé‡åŒ– ...

stats_collector.print_summary()
```

**è¾“å‡ºç¤ºä¾‹**:
```
============================================================
é‡åŒ–ç»Ÿè®¡æ‘˜è¦ (Quantization Statistics Summary)
============================================================

æ€»é€šé“æ•°: 10240
æ¯”ç‰¹åˆ†å¸ƒ:
  2-bit:   2048 é€šé“ (20.00%)
  3-bit:   2048 é€šé“ (20.00%)
  4-bit:   2048 é€šé“ (20.00%)
  6-bit:   2048 é€šé“ (20.00%)
  8-bit:   2048 é€šé“ (20.00%)

å¹³å‡æ¯”ç‰¹æ•°: 4.600 bits

æ¯å±‚ç»Ÿè®¡:
  Layer 0: avg=4.60 bits, importance_range=(0.234, 2.456)
  Layer 1: avg=4.55 bits, importance_range=(0.189, 2.678)
  ...
============================================================
```

## ðŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```python
from sparsegpt_enhanced import SparseGPT, QuantizationStats
import torch.nn as nn

# 1. åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨ï¼ˆå¯é€‰ï¼‰
stats = QuantizationStats()

# 2. ä¸ºæ¯ä¸€å±‚åˆ›å»º SparseGPT å®žä¾‹
layer = model.layers[0]
sparsegpt = SparseGPT(
    layer, 
    layer_name="layer_0",
    stats_collector=stats
)

# 3. é…ç½®é‡åŒ–å™¨
from quant import Quantizer
sparsegpt.quantizer = Quantizer()
sparsegpt.quantizer.configure(bits=4, perchannel=True, sym=True)

# 4. æ”¶é›†æ¿€æ´»ç»Ÿè®¡ï¼ˆå¤šä¸ªæ‰¹æ¬¡ï¼‰
for batch in dataloader:
    inp = batch['input']
    out = layer(inp)
    sparsegpt.add_batch(inp, out)

# 5. æ‰§è¡Œå‰ªæž + å¢žå¼ºé‡åŒ–
sparsegpt.fasterprune(
    sparsity=0.5,              # 50% å‰ªæžçŽ‡
    target_avg_bits=4.0,       # ç›®æ ‡å¹³å‡ 4-bit
    bit_allocation_method='quantile'  # åˆ†ä½æ•°æ–¹æ³•
)

# 6. æŸ¥çœ‹ç»Ÿè®¡
stats.print_summary()

# 7. é‡Šæ”¾èµ„æº
sparsegpt.free()
```

### å®Œæ•´ç¤ºä¾‹

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
# æµ‹è¯•å¢žå¼ºç‰ˆåŠŸèƒ½
python test_enhanced.py

# æˆ–åœ¨ OPT æ¨¡åž‹ä¸Šè¿è¡Œï¼ˆéœ€è¦ä¿®æ”¹ opt.pyï¼‰
python opt_enhanced.py facebook/opt-125m c4 --sparsity 0.5 --target_avg_bits 4.0
```

## ðŸ“Š æ€§èƒ½å¯¹æ¯”

### ç†è®ºåˆ†æž

| ç‰ˆæœ¬ | é‡åŒ–ç­–ç•¥ | å¹³å‡æ¯”ç‰¹ | æ¨¡åž‹å¤§å° | é¢„æœŸç²¾åº¦ |
|------|---------|---------|---------|---------|
| åŽŸç‰ˆ SparseGPT | å›ºå®š 4-bit | 4.0 | åŸºå‡† | åŸºå‡† |
| sparsegpt_toky | ç®€å•3æ¡£ (2/4/8) | ~4.7 | -6% | +2% |
| **å¢žå¼ºç‰ˆ** | ç²¾ç»†5æ¡£ (2/3/4/6/8) | **4.0** | **-10%** | **+3%** |

### ä¼˜åŠ¿

âœ… **æ›´å°çš„æ¨¡åž‹** - æ›´å¤šä½Žæ¯”ç‰¹æƒé‡  
âœ… **æ›´é«˜çš„ç²¾åº¦** - å…³é”®æƒé‡ä¿æŒé«˜æ¯”ç‰¹  
âœ… **çµæ´»æŽ§åˆ¶** - å¯ç²¾ç¡®è®¾å®šå¹³å‡æ¯”ç‰¹æ•°  
âœ… **æ™ºèƒ½åˆ†é…** - å¤šç»´åº¦è¯„ä¼°æ›´å‡†ç¡®  

## ðŸ”§ é«˜çº§é…ç½®

### è°ƒæ•´é‡è¦æ€§æƒé‡

ä¿®æ”¹ `compute_importance_scores()` ä¸­çš„æƒé‡ï¼š

```python
weights = {
    'activation': 0.30,   # æé«˜æ¿€æ´»çš„é‡è¦æ€§
    'hessian': 0.20,      # é™ä½Ž Hessian çš„æƒé‡
    'weight': 0.15,
    'output': 0.25,
    'stability': 0.10
}
```

### è‡ªå®šä¹‰æ¯”ç‰¹åˆ†é…

ä¿®æ”¹ `allocate_bits()` ä¸­çš„åˆ†ä½æ•°æˆ–æ¯”ç‰¹é€‰é¡¹ï¼š

```python
# æ›´æ¿€è¿›çš„ä½Žæ¯”ç‰¹åŒ–ï¼ˆ3æ¡£ï¼‰
quantiles = [0.3, 0.7]  # 30%-70%
bit_options = [2, 4, 8]

# æ›´ç²¾ç»†çš„åˆ†é…ï¼ˆ7æ¡£ï¼‰
quantiles = [0.15, 0.30, 0.45, 0.60, 0.75, 0.90]
bit_options = [2, 3, 4, 5, 6, 7, 8]
```

### åŠ¨æ€æ¯”ç‰¹é¢„ç®—

```python
# ä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒçš„æ¯”ç‰¹é¢„ç®—
for i, layer in enumerate(model.layers):
    if i < len(model.layers) // 2:
        target_bits = 3.5  # å‰åŠéƒ¨åˆ†ç”¨ä½Žæ¯”ç‰¹
    else:
        target_bits = 4.5  # åŽåŠéƒ¨åˆ†ç”¨é«˜æ¯”ç‰¹
    
    sparsegpt.fasterprune(
        sparsity=0.5,
        target_avg_bits=target_bits
    )
```

## ðŸ“ˆ å®žéªŒå»ºè®®

### 1. æ¶ˆèžå®žéªŒ

æµ‹è¯•å„ç»´åº¦é‡è¦æ€§çš„è´¡çŒ®ï¼š

```python
# åªç”¨æ¿€æ´»
weights = {'activation': 1.0, 'hessian': 0, ...}

# åªç”¨ Hessian
weights = {'activation': 0, 'hessian': 1.0, ...}

# æ¿€æ´» + Hessian
weights = {'activation': 0.5, 'hessian': 0.5, ...}
```

### 2. æ¯”ç‰¹é¢„ç®—æ‰«æ

æµ‹è¯•ä¸åŒå¹³å‡æ¯”ç‰¹æ•°çš„æ•ˆæžœï¼š

```python
for target_bits in [2.5, 3.0, 3.5, 4.0, 4.5, 5.0]:
    sparsegpt.fasterprune(
        sparsity=0.5,
        target_avg_bits=target_bits
    )
    # è¯„ä¼°ç²¾åº¦
    evaluate_model(model, test_loader)
```

### 3. æ–¹æ³•å¯¹æ¯”

```python
methods = ['quantile', 'budget']
for method in methods:
    sparsegpt.fasterprune(
        sparsity=0.5,
        target_avg_bits=4.0,
        bit_allocation_method=method
    )
    # å¯¹æ¯”ç»“æžœ
```

## ðŸ› è°ƒè¯•

å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼š

```python
# åœ¨ sparsegpt_enhanced.py é¡¶éƒ¨
DEBUG = True

# å°†æ‰“å°è¯¦ç»†çš„ä¸­é—´ç»“æžœå’Œæ¢¯åº¦ä¿¡æ¯
```

## ðŸ“š å‚è€ƒæ–‡çŒ®

1. **SparseGPT**: Frantar & Alistarh, 2023  
2. **GPTQ**: Frantar et al., 2022  
3. **OBS (Optimal Brain Surgeon)**: Hassibi & Stork, 1993  
4. **Mixed-Precision Quantization**: Wang et al., 2019  

## ðŸ¤ è´¡çŒ®

å¦‚æžœæœ‰ä»»ä½•é—®é¢˜æˆ–æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿Žæå‡ºï¼

## ðŸ“ ä¸‹ä¸€æ­¥è®¡åˆ’

- [ ] å®žçŽ°æ”¹è¿›3ï¼šåŠ¨æ€é€šé“ç¼©æ”¾é‡åŒ–
- [ ] å®žçŽ°æ”¹è¿›1ï¼šåŸºäºŽäº’ä¿¡æ¯çš„é‡åŒ–åˆ†ç»„
- [ ] é›†æˆåˆ°å®Œæ•´çš„è®­ç»ƒæµç¨‹
- [ ] åœ¨å¤§è§„æ¨¡æ¨¡åž‹ï¼ˆOPT-6.7B, LLaMA-7Bï¼‰ä¸ŠéªŒè¯

---

**ä½œè€…**: Toky  
**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025-10

