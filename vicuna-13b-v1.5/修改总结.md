# Vicuna MI 量化脚本修改总结

## 修改内容

### 1. 修复的问题

#### Bug修复
- **行165**: 修复了 `layer_name` 变量作用域问题，在循环内重新定义避免引用错误

#### 改进
- **行50-53**: 添加了自动检测序列长度的逻辑，支持不同配置的模型
- **行67-70**: 添加了模型架构检测，确保只支持 LLaMA/Vicuna 类架构，避免在不兼容的模型上运行

### 2. 创建的文件

1. **vicuna_mi.py** (已修改)
   - 专门用于 LLaMA/Vicuna 架构的 MI 量化脚本
   - 支持: LLaMA, Vicuna, Mistral, Yi, Qwen

2. **test_vicuna.sh** (新建)
   - 快速测试脚本
   - 默认配置: GPU 2,3, Vicuna-13b-v1.5, 50%稀疏, 4-bit量化

3. **README.md** (新建)
   - 详细的使用文档
   - 包含架构差异对比表
   - 多个使用示例
   - 常见问题解答

4. **使用说明.txt** (新建)
   - 简洁的快速参考
   - 10个关键要点
   - 常见错误和解决方法

5. **修改总结.md** (本文件)
   - 记录所有修改内容

## 关键技术要点

### LLaMA/Vicuna vs OPT 架构差异

| 组件 | OPT | LLaMA/Vicuna |
|------|-----|--------------|
| 层结构 | `model.decoder.layers` | `model.model.layers` |
| 嵌入层 | `embed_tokens` + `embed_positions` | `embed_tokens` (无位置) |
| 归一化 | `final_layer_norm` | `model.norm` |
| Attn输出 | `out_proj` | `o_proj` ⚠️ |
| FFN | `fc1`, `fc2` | `gate_proj`, `up_proj`, `down_proj` ⚠️ |

### 依赖项

**必需:**
- `sentencepiece` - LLaMA tokenizer 必需！

**安装命令:**
```bash
pip install sentencepiece
```

## 使用方法

### 快速开始

```bash
# 1. 确保安装 sentencepiece
pip install sentencepiece

# 2. 运行测试脚本
./test_vicuna.sh
```

### 手动运行

```bash
# Vicuna-13b-v1.5
python vicuna_mi.py \
    lmsys/vicuna-13b-v1.5 \
    c4 \
    --sparsity 0.5 \
    --wbits 4 \
    --target_avg_bits 4.0

# LLaMA-2-7b
python vicuna_mi.py \
    meta-llama/Llama-2-7b-hf \
    wikitext2 \
    --wbits 4

# 本地模型
python vicuna_mi.py \
    /path/to/your/vicuna-model \
    c4 \
    --sparsity 0.5 \
    --wbits 4
```

## 支持的模型

### ✅ 支持
- LLaMA (所有版本: 7B, 13B, 30B, 65B, 70B)
- Vicuna (所有版本)
- Mistral (7B, 8x7B)
- Yi (6B, 34B)
- Qwen (所有版本)
- 其他基于 LLaMA 架构的模型

### ❌ 不支持
- OPT (使用 `mutual_info_quantization/opt_mi.py`)
- BLOOM (需要另外适配)
- GPT-2 (需要另外适配)

## 常见问题

### Q1: ImportError: LlamaTokenizer requires the SentencePiece library

**原因:** 缺少 sentencepiece 库

**解决:**
```bash
pip install sentencepiece
```

### Q2: 不支持的模型架构错误

**原因:** 模型不是 LLaMA/Vicuna 架构

**解决:** 
- 检查模型类型: `print(model.config.model_type)`
- 使用对应的脚本 (OPT 用 `opt_mi.py`)

### Q3: CUDA out of memory

**解决方法:**
```bash
# 减少样本数
--nsamples 64

# 减少分组数
--n_groups 5

# 使用单GPU
export CUDA_VISIBLE_DEVICES=2
```

### Q4: 如何适配自定义 LLaMA 模型？

**答:** 无需修改！只要是基于 LLaMA 架构的模型，直接使用即可：

```bash
python vicuna_mi.py \
    /path/to/your/custom-llama-model \
    c4 \
    --sparsity 0.5 \
    --wbits 4
```

## 技术实现

### MI 量化流程
1. **激活收集**: 捕获前向传播中的激活值
2. **MI 计算**: 计算通道间的互信息矩阵
3. **谱聚类**: 基于 MI 相似度对通道分组
4. **自适应量化**: 不同分组使用不同位宽
5. **误差补偿**: 使用 Hessian 传播量化误差

### 核心模块
- `mutual_info.py`: MI 计算（快速近似）
- `channel_grouping.py`: 谱聚类分组
- `sparsegpt_mi.py`: 核心剪枝/量化逻辑

## GPU 设置

### 默认配置
脚本默认使用 **GPU 2 和 3**

### 修改方法

**方法1: 环境变量**
```bash
export CUDA_VISIBLE_DEVICES=0,1
python vicuna_mi.py ...
```

**方法2: 修改脚本**
编辑 `vicuna_mi.py` 第12-13行:
```python
if 'CUDA_VISIBLE_DEVICES' not in os.environ:
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # 改为你的GPU
```

## 输出说明

### 运行时输出
- 模型架构信息
- 每层处理进度
- MI 统计信息（每层）
- 分组效果
- 量化位宽分配

### 最终结果
- wikitext2 困惑度
- ptb 困惑度  
- c4 困惑度
- 压缩模型（如果指定 `--save`）

## 下一步建议

### 1. 性能优化
- 调整 `--n_groups` 找到最优分组数
- 调整 `--target_avg_bits` 平衡压缩率和性能
- 尝试不同的 `--sparsity` 值

### 2. 模型部署
```bash
# 保存压缩模型
python vicuna_mi.py ... --save ./compressed_model

# 加载使用
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("./compressed_model")
```

### 3. 适配其他模型
- **GLM-4**: 需要检查架构后单独适配
- **Qwen**: 已支持，直接使用
- **自定义模型**: 确认是 LLaMA 架构后直接使用

## 技术支持

如有问题，检查：
1. ✅ 已安装 `sentencepiece`
2. ✅ 模型是 LLaMA/Vicuna 架构
3. ✅ GPU 显存充足
4. ✅ 路径设置正确（sys.path）

参考文档:
- `README.md` - 详细文档
- `使用说明.txt` - 快速参考
- `mutual_info_quantization/README.md` - MI 算法原理

